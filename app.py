import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from streamlit_agraph import agraph, Node, Edge, Config
from fpdf import FPDF


st.set_page_config(page_title="BNS Legal Assistant", layout="wide")


st.markdown("""
<style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    h1 {
        color: #00ff41;
        font-family: 'Courier New', monospace;
    }
    .stTextArea textarea {
        background-color: #0e1117;
        color: #c9d1d9;
        border: 1px solid #30363d;
    }
    .stSuccess {
        background-color: #0e1117;
        border-left: 5px solid #00ff41;
    }
</style>
""", unsafe_allow_html=True)

st.title("Bharatiya Nyaya Sanhita (BNS) Legal Assistant")
st.caption("Offline AI System | Replacing IPC with BNS 2023 | 100% Privacy Preserved")


@st.cache_resource
def load_db():
    embeddings = HuggingFaceEmbeddings(model_name="./my_offline_model")
    db = Chroma(persist_directory="./bns_vector_db", embedding_function=embeddings)
    return db

db = load_db()
llm = ChatOllama(model="mistral") 



def render_knowledge_graph(section_id, offense_name):
    nodes = []
    edges = []
    
    
    nodes.append(Node(id=offense_name, label=offense_name[:20], size=25, shape="diamond", color="#FF4B4B"))
    nodes.append(Node(id=str(section_id), label=f"Sec {section_id}", size=15, color="#1E88E5"))
    edges.append(Edge(source=offense_name, target=str(section_id), label="Defined In"))
    

    concepts = ["Cognizable", "Non-Bailable", "Magistrate Trial"]
    for i, concept in enumerate(concepts):
        nodes.append(Node(id=concept, label=concept, size=10, color="#00C853"))
        edges.append(Edge(source=offense_name, target=concept, label="Attr"))

    config = Config(width=600, height=250, directed=True, nodeHighlightBehavior=True, highlightColor="#F7A7A6")
    return agraph(nodes=nodes, edges=edges, config=config)

def create_pdf(query, ai_response, sections):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    
    
    pdf.set_font("Arial", 'B', 16)
    pdf.cell(200, 10, txt="BNS Legal Opinion Report", ln=True, align='C')
    
    
    pdf.set_font("Arial", 'B', 12)
    
    clean_query = query.encode('latin-1', 'replace').decode('latin-1')
    pdf.multi_cell(0, 10, txt=f"Incident: {clean_query}\n")
    
    
    pdf.set_font("Arial", size=10)
    clean_response = ai_response.encode('latin-1', 'replace').decode('latin-1')
    pdf.multi_cell(0, 10, txt=f"AI Analysis:\n{clean_response}")
    
    
    pdf.ln(10)
    pdf.set_font("Arial", 'I', 8)
    pdf.cell(200, 10, txt="Generated by Offline-RAG System. Verified Secure.", ln=True, align='C')
    
    return pdf.output(dest='S').encode('latin-1')


system_prompt = (
    "You are an expert Indian Legal Advisor specialized in the Bharatiya Nyaya Sanhita (BNS) 2023. "
    "Use the following pieces of retrieved context to answer the question. "
    "If you don't know the answer, say that you don't know. "
    "\n\n"
    "RULES:\n"
    "1. Identify the specific BNS Section numbers relevant to the crime.\n"
    "2. Classify the offense as COGNIZABLE (Police can arrest) or NON-COGNIZABLE if possible.\n"
    "3. State the PUNISHMENT clearly.\n"
    "\n\n"
    "CONTEXT FROM BNS DATABASE:\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

st.sidebar.markdown("### System Status")
st.sidebar.success("Database: Connected (Local)")
st.sidebar.success("Model: Mistral (Offline)")
st.sidebar.info("Data Source: Official BNS Gazette 2023")
st.sidebar.markdown("---")
st.sidebar.markdown("### ðŸ’° ROI Calculator")

if 'tokens_saved' not in st.session_state:
    st.session_state.tokens_saved = 0

cost_saved = (st.session_state.tokens_saved / 1000) * 0.03 * 90 

col_a, col_b = st.sidebar.columns(2)
col_a.metric("Tokens Processed", f"{st.session_state.tokens_saved:,}")
col_b.metric("Money Saved (â‚¹)", f"â‚¹{cost_saved:.2f}")
st.sidebar.caption("vs. OpenAI GPT-4 API pricing")

col1, col2 = st.columns([1, 1])

with col1:
    st.info("Describe the Incident")
    user_query = st.text_area("Enter details", placeholder="e.g., My neighbor threatened to hit me...", height=150)
    analyze_btn = st.button("Analyze Incident (Offline)")

with col2:
    if analyze_btn and user_query:
        with st.spinner("Searching BNS Sections..."):
            try:
                docs = db.similarity_search(user_query, k=3)
                context_text = "\n\n".join([doc.page_content for doc in docs])
                
                formatted_prompt = prompt.format_messages(context=context_text, input=user_query)
                response = llm.invoke(formatted_prompt)
                
                st.session_state.tokens_saved += 500 # Approximate token usage per run
                
                st.success("Legal Analysis")
                st.markdown(response.content)

                st.warning("Source Sections (Evidence)")
                for doc in docs:
                    section_id = doc.metadata.get('Section', 'Unknown')
                    offense_name = doc.metadata.get('Section _name', 'N/A')
                    
                    with st.expander(f"Section {section_id}: {offense_name}"):
                        st.write(doc.page_content)
                        render_knowledge_graph(section_id, offense_name)

                pdf_bytes = create_pdf(user_query, response.content, docs)
                st.download_button(label="ðŸ“„ Download Official Legal Report", 
                                    data=pdf_bytes, 
                                    file_name="legal_report.pdf", 
                                    mime="application/pdf")
            
            except Exception as e:
                st.error(f"Error: {e}")